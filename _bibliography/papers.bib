---
---

@string{aps = {American Physical Society,}}

@InProceedings{icra21-xu,
  bibtex_show={true},
  selected={true},
  author = {Zifan Xu and Gauraang Dhamankar and Anirudh Nair and Xuesu Xiao and Garrett Warnell and Bo Liu and Zizhao Wang and Peter Stone},
  title = {APPLR: Adaptive Planner Parameter Learning from Reinforcement},
  booktitle = {Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA 2021)},
  location = {Xi'an, China},
  month = {June},
  year = {2021},
  abstract = {Classical navigation systems typically operate using a fixed set of hand-picked parameters (e.g. maximum speed, sampling rate, inflation radius, etc.) and require heavy expert re-tuning in order to work in new environments. To mitigate this requirement, it has been proposed to learn parameters for different contexts in a new environment using human demonstrations collected via teleoperation. However, learning from human demonstration limits deployment to the training environment, and limits overall performance to that of a potentially-suboptimal demonstrator. In this paper, we introduce APPLR, Adaptive Planner Parameter Learning from Reinforcement, which allows existing navigation systems to adapt to new scenarios by using a parameter selection scheme discovered via reinforcement learning (RL) in a wide variety of simulation environments. We evaluate APPLR on a robot in both simulated and physical experiments, and show that it can outperform both a fixed set of hand-tuned parameters and also a dynamic parameter tuning scheme learned from human demonstration.},
  wwwnote={<a href="https://www.youtube.com/watch?v=JKHTAowdGUk&t=26s">Video presentation</a><br><a href="https://www.cs.utexas.edu/~xiao/Research/APPL/APPL.html">Project webpage</a>}
}